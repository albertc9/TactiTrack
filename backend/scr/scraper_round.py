# check
import os
import requests
import pandas as pd
import time
import hashlib

# èµ›å­£ ID
SEASON_ID = "61627"
# ç›®æ ‡æ–‡ä»¶å¤¹
SAVE_DIR = f"backend/round/{SEASON_ID}"
os.makedirs(SAVE_DIR, exist_ok=True)

# API
API_URL_TEMPLATE = "https://www.sofascore.com/api/v1/unique-tournament/17/season/{}/events/round/{}"

# å¤±è´¥è½®æ¬¡åˆ—è¡¨
failed_rounds = []

def fetch_data(round_number):
    """è·å–æŒ‡å®šè½®æ¬¡çš„æ¯”èµ›æ•°æ®"""
    url = API_URL_TEMPLATE.format(SEASON_ID, round_number)
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Request failed: Round {round_number}, error: {e}")
        return None

def clean_data(data):
    """æ•´ç†æ¯”èµ›æ•°æ®ï¼Œä½¿å…¶æ ¼å¼æ¸…æ™°å¯è¯»"""
    if not data or "events" not in data:
        return []

    cleaned_data = []
    for event in data["events"]:
        match_info = {
            "Match ID": event.get("id"),
            "Round": event.get("roundInfo", {}).get("round", "N/A"),
            "Status": event.get("status", {}).get("description", "N/A"),
            "Start Timestamp": event.get("startTimestamp", "N/A"),
            "Home Team": event.get("homeTeam", {}).get("name", "N/A"),
            "Away Team": event.get("awayTeam", {}).get("name", "N/A"),
            "Home Score": event.get("homeScore", {}).get("current", "N/A"),
            "Away Score": event.get("awayScore", {}).get("current", "N/A"),
            "Final Result Only": event.get("finalResultOnly", "N/A"),
            "Has XG Data": event.get("hasXg", "N/A"),
            "Slug": event.get("slug", "N/A"),
        }
        cleaned_data.append(match_info)

    return cleaned_data

def calculate_hash(data):
    """è®¡ç®—æ•°æ®çš„å“ˆå¸Œå€¼"""
    return hashlib.md5(pd.DataFrame(data).to_csv(index=False).encode()).hexdigest()

def get_existing_hash(file_path):
    """è·å–å·²å­˜åœ¨çš„ CSV æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
        return None
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def save_to_csv(data, round_number):
    """æ£€æŸ¥å“ˆå¸Œå€¼å¹¶å†³å®šæ˜¯å¦æ›´æ–° CSV æ–‡ä»¶"""
    file_path = os.path.join(SAVE_DIR, f"{round_number}.csv")

    if not data:
        print(f"âŒ No data found: Round {round_number}, creating an empty CSV.")
        pd.DataFrame().to_csv(file_path, index=False, encoding="utf-8")
        return

    new_hash = calculate_hash(data)
    old_hash = get_existing_hash(file_path)

    if old_hash == new_hash:
        print(f"âœ… Round {round_number} data unchanged, skipping update.")
    else:
        pd.DataFrame(data).to_csv(file_path, index=False, encoding="utf-8")
        print(f"ğŸ”„ Round {round_number} updated: {file_path}")

def find_not_started_rounds():
    """æ‰«æ CSV æ–‡ä»¶ï¼Œæ‰¾å‡ºæ‰€æœ‰åŒ…å« Not Started æˆ– Postponed çŠ¶æ€çš„è½®æ¬¡"""
    not_started_rounds = set()
    
    for file_name in os.listdir(SAVE_DIR):
        if not file_name.endswith(".csv"):
            continue

        round_number = int(file_name.split(".")[0])
        file_path = os.path.join(SAVE_DIR, file_name)
        df = pd.read_csv(file_path)

        if "Status" in df.columns and "Round" in df.columns:
            if any(df["Status"].eq("Not Started")) or any(df["Status"].eq("Postponed")):
                not_started_rounds.add(round_number)

    return sorted(not_started_rounds)

def compute_update_rounds(not_started_rounds):
    """åŸºäº Not Started è½®æ¬¡è®¡ç®—éœ€è¦æ›´æ–°çš„è½®æ¬¡ï¼ˆå‰å 2 è½®ï¼‰"""
    update_rounds = set()

    for round_number in not_started_rounds:
        for offset in range(-2, 3):  # å‘å‰2è½®ï¼Œå‘å2è½®
            new_round = round_number + offset
            if 1 <= new_round <= 38:  # é™åˆ¶åœ¨ 1-38 è½®èŒƒå›´å†…
                update_rounds.add(new_round)

    return sorted(update_rounds)

def retry_failed_rounds():
    """é‡è¯•å¤±è´¥çš„è½®æ¬¡"""
    global failed_rounds
    max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

    for attempt in range(1, max_retries + 1):
        if not failed_rounds:
            return  # å¦‚æœæ²¡æœ‰å¤±è´¥çš„è½®æ¬¡ï¼Œæå‰è¿”å›

        print(f"\nğŸ”„ Retrying {len(failed_rounds)} failed rounds (Attempt {attempt}/{max_retries})...\n")

        remaining_failures = []
        for round_number in failed_rounds:
            print(f"Retrying round {round_number}...")
            data = fetch_data(round_number)
            formatted_data = clean_data(data)
            
            if formatted_data or os.path.exists(os.path.join(SAVE_DIR, f"{round_number}.csv")):
                save_to_csv(formatted_data, round_number)
            else:
                print(f"âŒ Round {round_number} still failed, will retry later.")
                remaining_failures.append(round_number)

        failed_rounds = remaining_failures  # æ›´æ–°å¤±è´¥åˆ—è¡¨
        if failed_rounds:
            time.sleep(5)  # ä¼‘çœ  5 ç§’å†è¯•ä¸‹ä¸€è½®
        else:
            print("âœ… All failed rounds have been successfully fetched!")
            return

    if failed_rounds:
        print(f"âš ï¸ Some rounds still failed after retries: {failed_rounds}")

def main():
    print("ğŸ” Scanning for Not Started and Postponed rounds...")
    not_started_rounds = find_not_started_rounds()
    print(f"ğŸ“ Not Started/Postponed Rounds: {not_started_rounds}")

    update_rounds = compute_update_rounds(not_started_rounds)
    print(f"ğŸ”„ Rounds to update: {update_rounds}")

    for round_number in update_rounds:
        print(f"ğŸ”„ Fetching round {round_number} data...")
        data = fetch_data(round_number)
        formatted_data = clean_data(data)
        
        if not formatted_data:
            failed_rounds.append(round_number)  # è®°å½•å¤±è´¥çš„è½®æ¬¡
        save_to_csv(formatted_data, round_number)

    # è¿è¡Œå®Œæ‰€æœ‰è½®æ¬¡åï¼Œé‡æ–°å°è¯•å¤±è´¥çš„è½®æ¬¡
    retry_failed_rounds()

if __name__ == "__main__":
    main()
