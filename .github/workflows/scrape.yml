name: Daily Scheduled Web Scraping and Commit

on:
  schedule:
    - cron: '0 22 * * *'  # Runs daily at 22:00 UTC (06:00 Beijing Time)
    - cron: '10 22 * * *'  # Runs daily at 22:10 UTC (06:10 Beijing Time)
    - cron: '20 22 * * *'  # Runs daily at 22:20 UTC (06:20 Beijing Time)
    - cron: '30 22 * * *'  # Runs daily at 22:30 UTC (06:30 Beijing Time)
  workflow_dispatch:  # Allows manual trigger

jobs:
  scrape_and_commit:
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🐍 Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'  # Match the Python version used in your venv

      - name: 📦 Install dependencies
        run: pip install -r backend/requirements.txt  # Updated path

      - name: 🚀 Run scraper scripts
        run: python backend/run_scrapers.py  # Runs the main scraper script

      - name: 📝 Configure Git user
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

      - name: 🔍 Check for changes
        id: check_changes
        run: |
          git add .
          if git diff --staged --quiet; then
            echo "NO_CHANGES=true" >> $GITHUB_ENV
          else
            echo "NO_CHANGES=false" >> $GITHUB_ENV
          fi

      - name: 🔄 Commit and push changes (if any)
        if: env.NO_CHANGES == 'false'
        run: |
          git commit -m "📌 Auto-update scraped data $(date '+%Y-%m-%d %H:%M:%S')"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
